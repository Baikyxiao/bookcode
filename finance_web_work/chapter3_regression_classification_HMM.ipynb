{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e348de9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用均方误差评估本章的算法\n",
    "##########################监督学习###############################################\n",
    "#广义线性模型：线性回归、岭回归、套索回归、对数几率回归\n",
    "#K近邻（简单分类方法，将近邻的多个标签中出现次数最多的分配给该点）\n",
    "#朴素贝叶斯，多项式朴素贝叶斯、高斯朴素贝叶斯\n",
    "#决策树（从特征中学习，生成简单规则分割训练集，预测未知标签）\n",
    "#支持向量机"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1091d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear regression\n",
      "mean R2:0.70 (+/- 0.24)\n",
      "MSE: 24.157283417962216\n",
      "ridge regression\n",
      "mean R2: 0.69 (+/- 0.25)\n",
      "MSE: 24.35252724752238\n",
      "lasso regression\n",
      "mean R2: 0.68 (+/- 0.24)\n",
      "MSE: 25.386084654728126\n",
      "decision tree regression\n",
      "mean R2: 0.79 (+/- 0.14)\n",
      "MSE: 17.02118577075099\n",
      "random forest regression\n",
      "mean R2: 0.87 (+/- 0.14)\n",
      "MSE: 9.988459351778657\n",
      "linear support vector machine\n",
      "mean R2: 0.67 (+/- 0.31)\n",
      "MSE: 26.350858859089254\n",
      "support vector machine rbf\n",
      "mean R2: 0.19 (+/- 0.32)\n",
      "MSE: 67.50078365079564\n",
      "knn\n",
      "mean R2: 0.55 (+/- 0.21)\n",
      "MSE: 36.57860158102767\n"
     ]
    }
   ],
   "source": [
    "###########################评估各回归算法##########################################\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "df = pd.read_csv('./data/housing.csv',sep = ',',header=None)    #导入数据\n",
    "df = df.iloc[np.random.permutation(len(df))]                     #打乱原数据顺序\n",
    "X = df[df.columns[:-1]].values\n",
    "Y = df[df.columns[-1]].values\n",
    "#分别对线性回归、岭回归、套索回归和支持向量机（SVM）计算均方误差(MSE)和判定系数(R^2)\n",
    "cv = 10                                                         #线性回归\n",
    "print('linear regression')\n",
    "lin = LinearRegression()\n",
    "scores = model_selection.cross_val_score(lin,X,Y,cv = cv)\n",
    "print('mean R2:%0.2f (+/- %0.2f)'%(scores.mean(),scores.std()*2))\n",
    "predicted = model_selection.cross_val_predict(lin,X,Y,cv=cv)\n",
    "print('MSE:',mean_squared_error(Y,predicted))\n",
    "\n",
    "print('ridge regression')                         #岭回归\n",
    "ridge = Ridge(alpha=1.0)\n",
    "scores = model_selection.cross_val_score(ridge, X, Y, cv=cv)\n",
    "print((\"mean R2: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2)))\n",
    "predicted = model_selection.cross_val_predict(ridge, X,Y, cv=cv)\n",
    "print('MSE:',mean_squared_error(Y,predicted))\n",
    "\n",
    "print('lasso regression')                        #套索回归\n",
    "lasso = Lasso(alpha=0.1)\n",
    "scores = model_selection.cross_val_score(lasso, X, Y, cv=cv)\n",
    "print((\"mean R2: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2)))\n",
    "predicted = model_selection.cross_val_predict(lasso, X,Y, cv=cv)\n",
    "print('MSE:',mean_squared_error(Y,predicted))\n",
    "\n",
    "print('decision tree regression')               #决策树\n",
    "tree = DecisionTreeRegressor(random_state=0)\n",
    "scores = model_selection.cross_val_score(tree, X, Y, cv=cv)\n",
    "print((\"mean R2: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2)))\n",
    "predicted = model_selection.cross_val_predict(tree, X,Y, cv=cv)\n",
    "print('MSE:',mean_squared_error(Y,predicted))\n",
    " \n",
    "print('random forest regression')              #随机森林\n",
    "forest = RandomForestRegressor(n_estimators=50, max_depth=None,min_samples_split=2, \n",
    "                               random_state=0)\n",
    "scores = model_selection.cross_val_score(forest, X, Y, cv=cv)\n",
    "print((\"mean R2: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2)))\n",
    "predicted = model_selection.cross_val_predict(forest, X,Y, cv=cv)\n",
    "print('MSE:',mean_squared_error(Y,predicted))\n",
    "\n",
    "print('linear support vector machine')        #线性内核的支持向量机\n",
    "svm_lin = svm.SVR(epsilon=0.2,kernel='linear',C=1)\n",
    "scores = model_selection.cross_val_score(svm_lin, X, Y, cv=cv)\n",
    "print((\"mean R2: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2)))\n",
    "predicted = model_selection.cross_val_predict(svm_lin, X,Y, cv=cv)\n",
    "print('MSE:',mean_squared_error(Y,predicted))\n",
    "\n",
    "print('support vector machine rbf')            #支持向量机\n",
    "clf = svm.SVR(epsilon=0.2,kernel='rbf',C=1.)\n",
    "scores = model_selection.cross_val_score(clf, X, Y, cv=cv)\n",
    "print((\"mean R2: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2)))\n",
    "predicted = model_selection.cross_val_predict(clf, X,Y, cv=cv)\n",
    "print('MSE:',mean_squared_error(Y,predicted))\n",
    "\n",
    "print('knn')                                   #K近邻\n",
    "knn = KNeighborsRegressor()\n",
    "scores = model_selection.cross_val_score(knn, X, Y, cv=cv)\n",
    "print((\"mean R2: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2)))\n",
    "predicted = model_selection.cross_val_predict(knn, X,Y, cv=cv)\n",
    "print('MSE:',mean_squared_error(Y,predicted))\n",
    "\n",
    "#R^2（判定系数）的值越接近1，模型的预测效果越好\n",
    "#随机森林算法训练的模型对数据的拟合程度最高，线性内核支持向量机比普通支持向量机提升效果明显"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e23918a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature selection on linear regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:72: FutureWarning: Pass n_features_to_select=4 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.57 (+/- 0.44)\n",
      "MSE: 34.02605152525706\n",
      "feature selection ridge regression\n",
      "R2: 0.57 (+/- 0.44)\n",
      "MSE: 34.09757261001352\n",
      "feature selection on lasso regression\n",
      "R2: 0.65 (+/- 0.23)\n",
      "MSE: 28.07040398875744\n",
      "feature selection on decision tree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:72: FutureWarning: Pass n_features_to_select=4 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n",
      "E:\\anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:72: FutureWarning: Pass n_features_to_select=4 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n",
      "E:\\anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:72: FutureWarning: Pass n_features_to_select=4 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.78 (+/- 0.19)\n",
      "MSE: 17.78276679841897\n",
      "feature selection on random forest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:72: FutureWarning: Pass n_features_to_select=4 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.84 (+/- 0.19)\n",
      "MSE: 12.80952382608696\n",
      "feature selection on linear support vector machine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:72: FutureWarning: Pass n_features_to_select=4 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.56 (+/- 0.46)\n",
      "MSE: 26.350858859089254\n"
     ]
    }
   ],
   "source": [
    "#选取特征，递归特征消除法（只选取具有最大绝对权重的特征）\n",
    "#sklearn库中内置了函数RFE\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "best_features = 4                                #设置最佳特征的数量为4\n",
    "\n",
    "print('feature selection on linear regression')  #线性回归\n",
    "rfe_lin = RFE(lin,best_features).fit(X,Y)        #调用RFE函数计算\n",
    "mask = np.array(rfe_lin.support_)                #计算在RFE下模型的判定系数以及均方误差\n",
    "scores = model_selection.cross_val_score(lin, X[:,mask], Y, cv=cv)\n",
    "print((\"R2: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2)))\n",
    "predicted = model_selection.cross_val_predict(lin, X[:,mask],Y, cv=cv)\n",
    "print('MSE:',mean_squared_error(Y,predicted))\n",
    "\n",
    "print('feature selection ridge regression')      #岭回归\n",
    "rfe_ridge = RFE(ridge,best_features).fit(X,Y)\n",
    "mask = np.array(rfe_ridge.support_)\n",
    "scores = model_selection.cross_val_score(ridge, X[:,mask], Y, cv=cv)\n",
    "print((\"R2: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2)))\n",
    "predicted = model_selection.cross_val_predict(ridge, X[:,mask],Y, cv=cv)\n",
    "print('MSE:',mean_squared_error(Y,predicted))\n",
    "\n",
    "print('feature selection on lasso regression')    #套索回归\n",
    "rfe_lasso = RFE(lasso,best_features).fit(X,Y)\n",
    "mask = np.array(rfe_lasso.support_)\n",
    "scores = model_selection.cross_val_score(lasso, X[:,mask], Y, cv=cv)\n",
    "print((\"R2: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2)))\n",
    "predicted = model_selection.cross_val_predict(lasso, X[:,mask],Y, cv=cv)\n",
    "print('MSE:',mean_squared_error(Y,predicted))\n",
    " \n",
    "print('feature selection on decision tree')       #决策树\n",
    "rfe_tree = RFE(tree,best_features).fit(X,Y)\n",
    "mask = np.array(rfe_tree.support_)\n",
    "scores = model_selection.cross_val_score(tree, X[:,mask], Y, cv=cv)\n",
    "print((\"R2: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2)))\n",
    "predicted = model_selection.cross_val_predict(tree, X[:,mask],Y, cv=cv)\n",
    "print('MSE:',mean_squared_error(Y,predicted))\n",
    "\n",
    "print('feature selection on random forest')       #随机森林\n",
    "rfe_forest = RFE(forest,best_features).fit(X,Y)\n",
    "mask = np.array(rfe_forest.support_)\n",
    "scores = model_selection.cross_val_score(forest, X[:,mask], Y, cv=cv)\n",
    "print((\"R2: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2)))\n",
    "predicted = model_selection.cross_val_predict(forest, X[:,mask],Y, cv=cv)\n",
    "print('MSE:',mean_squared_error(Y,predicted))\n",
    "\n",
    "print('feature selection on linear support vector machine')#支持向量机\n",
    "rfe_svm = RFE(svm_lin,best_features).fit(X,Y)\n",
    "mask = np.array(rfe_svm.support_)\n",
    "scores = model_selection.cross_val_score(svm_lin, X[:,mask], Y, cv=cv)\n",
    "print((\"R2: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2)))\n",
    "predicted = model_selection.cross_val_predict(svm_lin, X,Y, cv=cv)\n",
    "print('MSE:',mean_squared_error(Y,predicted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2062e652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0      1  2  3      4     5      6\n",
      "0  vhigh  vhigh  2  2  small   low  unacc\n",
      "1  vhigh  vhigh  2  2  small   med  unacc\n",
      "2  vhigh  vhigh  2  2  small  high  unacc\n",
      "3  vhigh  vhigh  2  2    med   low  unacc\n",
      "4  vhigh  vhigh  2  2    med   med  unacc\n",
      "{'high': 0, 'low': 1, 'med': 2, 'vhigh': 3}\n",
      "      0  1  2  3  4  5  6\n",
      "161   3  0  1  2  0  0  2\n",
      "1077  2  0  3  2  0  1  2\n",
      "1039  2  0  2  1  1  2  0\n",
      "23    3  3  0  2  1  0  2\n",
      "93    3  3  3  1  1  1  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "E:\\anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          method       acc      good     unacc     vgood\n",
      "0  linear support vector machine  0.263566  0.000000  0.845331  0.000000\n",
      "1     rbf support vector machine  0.994805  0.992701  0.999174  0.992248\n",
      "2    poly support vector machine  0.791501  0.842105  0.937526  0.809917\n",
      "3                  decision tree  0.966234  0.942857  0.991304  0.977099\n",
      "4                  random forest  0.962677  0.942029  0.991701  0.977099\n",
      "5                    naive bayes  0.040506  0.000000  0.825983  0.000000\n",
      "6            logistic regression  0.239583  0.000000  0.822616  0.266667\n",
      "7           k nearest neighbours  0.779292  0.500000  0.953100  0.686275\n",
      "                          method       acc      good     unacc     vgood\n",
      "0  linear support vector machine  0.177083  0.000000  0.980165  0.000000\n",
      "1     rbf support vector machine  0.997396  0.985507  0.999174  0.984615\n",
      "2    poly support vector machine  0.776042  0.811594  0.948760  0.753846\n",
      "3                  decision tree  0.968750  0.956522  0.989256  0.984615\n",
      "4                  random forest  0.973958  0.942029  0.987603  0.984615\n",
      "5                    naive bayes  0.020833  0.000000  0.998347  0.000000\n",
      "6            logistic regression  0.179688  0.000000  0.919835  0.215385\n",
      "7           k nearest neighbours  0.744792  0.376812  0.990909  0.538462\n",
      "                          method       acc      good     unacc     vgood\n",
      "0  linear support vector machine  0.515152  0.000000  0.743108  0.000000\n",
      "1     rbf support vector machine  0.992228  1.000000  0.999174  1.000000\n",
      "2    poly support vector machine  0.807588  0.875000  0.926554  0.875000\n",
      "3                  decision tree  0.963731  0.929577  0.993361  0.969697\n",
      "4                  random forest  0.951654  0.942029  0.995833  0.969697\n",
      "5                    naive bayes  0.727273  0.000000  0.704373  0.000000\n",
      "6            logistic regression  0.359375  0.000000  0.743984  0.350000\n",
      "7           k nearest neighbours  0.817143  0.742857  0.918070  0.945946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "acc       384\n",
       "good       69\n",
       "unacc    1210\n",
       "vgood      65\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "df = pd.read_csv('./data/data_cars.csv',header = None)   #导入数据（描述汽车特点的数据集）\n",
    "for i in range(len(df.columns)):                         #将数据转化为float类型\n",
    "    df[i] = df[i].astype('category')\n",
    "print(df.head())\n",
    "#数据中的数据是相应的描述，所以为了处理要把特征值转换为数字\n",
    "map0 = dict(zip(df[1].cat.categories,range(len(df[0].cat.categories))))\n",
    "#zip() 函数用于将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的列表。\n",
    "#cat()函数用于合并series\n",
    "#category是pandas中定义的一个数据类型，相当于R中的因子。可以对特点的类型数据进行按照自己的意愿进行排序\n",
    "print(map0)\n",
    "map1 = dict( list(zip( df[1].cat.categories, list(range( len(df[1].cat.categories ))))))\n",
    "map2 = dict( list(zip( df[2].cat.categories, list(range( len(df[2].cat.categories ))))))\n",
    "map3 = dict( list(zip( df[3].cat.categories, list(range( len(df[3].cat.categories ))))))\n",
    "map4 = dict( list(zip( df[4].cat.categories, list(range( len(df[4].cat.categories ))))))\n",
    "map5 = dict( list(zip( df[5].cat.categories, list(range( len(df[5].cat.categories ))))))\n",
    "map6 = dict( list(zip( df[6].cat.categories, list(range( len(df[6].cat.categories ))))))\n",
    "\n",
    "cat_cols = df.select_dtypes(['category']).columns          #选择所有category类型的数据\n",
    "df[cat_cols] = df[cat_cols].apply(lambda x:x.cat.codes)\n",
    "\n",
    "df = df.iloc[np.random.permutation(len(df))]               #打乱原数据顺序\n",
    "print(df.head())\n",
    "#f值、准确率、召回率用于评估算法的正确性\n",
    "df_f1 = pd.DataFrame(columns=['method']+sorted(map6, key=map6.get))\n",
    "df_precision = pd.DataFrame(columns=['method']+sorted(map6, key=map6.get))\n",
    "df_recall = pd.DataFrame(columns=['method']+sorted(map6, key=map6.get))\n",
    "#定义函数，将类别标签向量Y和特征向量分开（因为需要计算和保存所有方法分类效果的度量指标）\n",
    "def CalcMeasures(method,y_pred,y_true,df_f1=df_f1\n",
    "                 ,df_precision=df_precision,df_recall=df_recall):\n",
    "\n",
    "    df_f1.loc[len(df_f1)] = [method]+list(f1_score(y_pred,y_true,average=None))\n",
    "    df_precision.loc[len(df_precision)] = [method]+list(precision_score(y_pred,y_true,average=None))\n",
    "    df_recall.loc[len(df_recall)] = [method]+list(recall_score(y_pred,y_true,average=None))\n",
    " \n",
    "X = df[df.columns[:-1]].values\n",
    "Y = df[df.columns[-1]].values\n",
    "#接下来计算各种算法的f值、准确率、召回率\n",
    "cv = 10                                           #线性内核支持向量机\n",
    "method = 'linear support vector machine'\n",
    "clf = svm.SVC(kernel = 'linear',C=50)\n",
    "y_pred = model_selection.cross_val_predict(clf,X,Y,cv=cv)\n",
    "CalcMeasures(method,y_pred,Y)\n",
    "\n",
    "method = 'rbf support vector machine'            #径向基内核支持向量机         \n",
    "clf = svm.SVC(kernel='rbf',C=50)\n",
    "y_pred = model_selection.cross_val_predict(clf, X,Y, cv=cv)\n",
    "CalcMeasures(method,y_pred,Y)\n",
    "\n",
    "method = 'poly support vector machine'            #rbf内核支持向量机\n",
    "clf = svm.SVC(kernel='poly',C=50)\n",
    "y_pred = model_selection.cross_val_predict(clf, X,Y, cv=cv)\n",
    "CalcMeasures(method,y_pred,Y)\n",
    "\n",
    "method = 'decision tree'                          #决策树\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "y_pred = model_selection.cross_val_predict(clf, X,Y, cv=cv)\n",
    "CalcMeasures(method,y_pred,Y)\n",
    "\n",
    "method = 'random forest'                           #随机森林\n",
    "clf = RandomForestClassifier(n_estimators=50,random_state=0,max_features=None)\n",
    "y_pred = model_selection.cross_val_predict(clf, X,Y, cv=cv)\n",
    "CalcMeasures(method,y_pred,Y)\n",
    "\n",
    "method = 'naive bayes'                            #朴素贝叶斯\n",
    "clf = MultinomialNB()\n",
    "y_pred = model_selection.cross_val_predict(clf, X,Y, cv=cv)\n",
    "CalcMeasures(method,y_pred,Y)\n",
    "\n",
    "method = 'logistic regression'                     #逻辑回归\n",
    "clf = LogisticRegression()\n",
    "y_pred = model_selection.cross_val_predict(clf, X,Y, cv=cv)\n",
    "CalcMeasures(method,y_pred,Y)\n",
    "\n",
    "method = 'k nearest neighbours'                    #K临近\n",
    "clf = KNeighborsClassifier(weights='distance',n_neighbors=5)\n",
    "y_pred = model_selection.cross_val_predict(clf, X,Y, cv=cv)\n",
    "CalcMeasures(method,y_pred,Y)\n",
    "\n",
    "print(df_f1)\n",
    "print(df_precision)\n",
    "print(df_recall)\n",
    "\n",
    "labels_counts= df[6].value_counts()\n",
    "pd.Series(map6).map(labels_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa6fc1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viterbi sequence: (0.004445279999999999, ' sequence: ', [0, 1, 0, 0])\n",
      "max prob sequence: 0100\n",
      "Estimated initial probabilities: [1. 0.]\n",
      "Estimated state transition probabililities: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Estimated observation probabililities: [[1.         0.         0.        ]\n",
      " [0.         0.38196618 0.61803382]]\n"
     ]
    }
   ],
   "source": [
    "################################隐马尔可夫########################################\n",
    "import numpy as np\n",
    "from copy import copy\n",
    "\n",
    "class HMM:\n",
    "    def __init__(self,pi,A,B):    #定义初始化pi（初始状态的概率分布）,A(状态转移矩阵)B（观察概率矩阵）的的值\n",
    "        self.pi = pi\n",
    "        self.A = A\n",
    "        self.B = B\n",
    "        \n",
    "    def MostLikelyStateSequence(self,observations):\n",
    "        \n",
    "        N = self.A.shape[0]\n",
    "        T = len(observations)\n",
    "        sequences = [str(i) for i in range(N)]\n",
    "        probs = np.array([self.pi[i]*self.B[i,observations[0]] for i in range(N)])\n",
    "        print(probs)\n",
    "        for i in range(1,T):\n",
    "            newsequences = []\n",
    "            newprobs = np.array([])\n",
    "            for s in range(len(sequences)):\n",
    "                for j in range(N):\n",
    "                    newsequences.append(sequences[s]+str(j))\n",
    "                    bef = int(sequences[s][-1])\n",
    "                    tTpprob = probs[s]*self.A[bef,j]*self.B[j,observations[i]]\n",
    "                    newprobs = np.append(newprobs,[tTpprob]) \n",
    "                    print(sequences[s]+str(j),'-',tTpprob)\n",
    "            sequences = newsequences\n",
    "            probs = newprobs\n",
    "        return max((probs[i],sequences[i]) for i in range(len(sequences)))\n",
    "            \n",
    "    def ViterbiSequence(self,observations):\n",
    "        deltas = [{}]\n",
    "        seq = {}\n",
    "        N = self.A.shape[0]\n",
    "        states = [i for i in range(N)]\n",
    "        T = len(observations)\n",
    "        #initialization\n",
    "        for s in states:\n",
    "            deltas[0][s] = self.pi[s]*self.B[s,observations[0]]\n",
    "            seq[s] = [s]\n",
    "        #compute Viterbi\n",
    "        for t in range(1,T):\n",
    "            deltas.append({})\n",
    "            newseq = {}\n",
    "            for s in states:\n",
    "                (delta,state) = max((deltas[t-1][s0]*self.A[s0,s]*self.B[s,observations[t]],s0) for s0 in states)\n",
    "                deltas[t][s] = delta\n",
    "                newseq[s] = seq[state] + [s]\n",
    "            seq = newseq\n",
    "            \n",
    "        (delta,state) = max((deltas[T-1][s],s) for s in states)\n",
    "        return  delta,' sequence: ', seq[state]\n",
    "        \n",
    "    def maxProbSequence(self,observations):\n",
    "        N = self.A.shape[0]\n",
    "        states = [i for i in range(N)]\n",
    "        T = len(observations)\n",
    "        M = self.B.shape[1]\n",
    "        # alpha_t(i) = P(O_1 O_2 ... O_t, q_t = S_i | hmm)\n",
    "        # Initialize alpha\n",
    "        alpha = np.zeros((N,T))\n",
    "        c = np.zeros(T) #scale factors\n",
    "        alpha[:,0] = pi.T * self.B[:,observations[0]]\n",
    "        c[0] = 1.0/np.sum(alpha[:,0])\n",
    "        alpha[:,0] = c[0] * alpha[:,0]\n",
    "        # Update alpha for each observation step\n",
    "        for t in range(1,T):\n",
    "            alpha[:,t] = np.dot(alpha[:,t-1].T, self.A).T * self.B[:,observations[t]]\n",
    "            c[t] = 1.0/np.sum(alpha[:,t])\n",
    "            alpha[:,t] = c[t] * alpha[:,t]\n",
    "\n",
    "        # beta_t(i) = P(O_t+1 O_t+2 ... O_T | q_t = S_i , hmm)\n",
    "        # Initialize beta\n",
    "        beta = np.zeros((N,T))\n",
    "        beta[:,T-1] = 1\n",
    "        beta[:,T-1] = c[T-1] * beta[:,T-1]\n",
    "        # Update beta backwards froT end of sequence\n",
    "        for t in range(len(observations)-1,0,-1):\n",
    "            beta[:,t-1] = np.dot(self.A, (self.B[:,observations[t]] * beta[:,t]))\n",
    "            beta[:,t-1] = c[t-1] * beta[:,t-1]\n",
    "            \n",
    "        norm = np.sum(alpha[:,T-1])\n",
    "        seq = ''\n",
    "        for t in range(T):\n",
    "            g,state = max(((beta[i,t]*alpha[i,t])/norm,i) for i in states)\n",
    "            seq +=str(state)\n",
    "            \n",
    "        return seq\n",
    "        \n",
    "    def simulate(self,time):\n",
    "\n",
    "        def drawFromNormal(probs):\n",
    "            return np.where(np.random.multinomial(1,probs) == 1)[0][0]\n",
    "\n",
    "        observations = np.zeros(time)\n",
    "        states = np.zeros(time)\n",
    "        states[0] = drawFromNormal(self.pi)\n",
    "        observations[0] = drawFromNormal(self.B[states[0],:])\n",
    "        for t in range(1,time):\n",
    "            states[t] = drawFromNormal(self.A[states[t-1],:])\n",
    "            observations[t] = drawFromNormal(self.B[states[t],:])\n",
    "        return observations,states\n",
    "\n",
    "\n",
    "    def train(self,observations,criterion):\n",
    "\n",
    "        N = self.A.shape[0]\n",
    "        T = len(observations)\n",
    "        M = self.B.shape[1]\n",
    "\n",
    "        A = self.A\n",
    "        B = self.B\n",
    "        pi = copy(self.pi)\n",
    "        \n",
    "        convergence = False\n",
    "        while not convergence:\n",
    "\n",
    "            # alpha_t(i) = P(O_1 O_2 ... O_t, q_t = S_i | hmm)\n",
    "            # Initialize alpha\n",
    "            alpha = np.zeros((N,T))\n",
    "            c = np.zeros(T) #scale factors\n",
    "            alpha[:,0] = pi.T * self.B[:,observations[0]]\n",
    "            c[0] = 1.0/np.sum(alpha[:,0])\n",
    "            alpha[:,0] = c[0] * alpha[:,0]\n",
    "            # Update alpha for each observation step\n",
    "            for t in range(1,T):\n",
    "                alpha[:,t] = np.dot(alpha[:,t-1].T, self.A).T * self.B[:,observations[t]]\n",
    "                c[t] = 1.0/np.sum(alpha[:,t])\n",
    "                alpha[:,t] = c[t] * alpha[:,t]\n",
    "\n",
    "            #P(O=O_0,O_1,...,O_T-1 | hmm)\n",
    "            P_O = np.sum(alpha[:,T-1])\n",
    "            # beta_t(i) = P(O_t+1 O_t+2 ... O_T | q_t = S_i , hmm)\n",
    "            # Initialize beta\n",
    "            beta = np.zeros((N,T))\n",
    "            beta[:,T-1] = 1\n",
    "            beta[:,T-1] = c[T-1] * beta[:,T-1]\n",
    "            # Update beta backwards froT end of sequence\n",
    "            for t in range(len(observations)-1,0,-1):\n",
    "                beta[:,t-1] = np.dot(self.A, (self.B[:,observations[t]] * beta[:,t]))\n",
    "                beta[:,t-1] = c[t-1] * beta[:,t-1]\n",
    "\n",
    "            gi = np.zeros((N,N,T-1));\n",
    "\n",
    "            for t in range(T-1):\n",
    "                for i in range(N):\n",
    "                    \n",
    "                    gamma_num = alpha[i,t] * self.A[i,:] * self.B[:,observations[t+1]].T * \\\n",
    "                            beta[:,t+1].T\n",
    "                    gi[i,:,t] = gamma_num / P_O\n",
    "  \n",
    "            # gamma_t(i) = P(q_t = S_i | O, hmm)\n",
    "            gamma = np.squeeze(np.sum(gi,axis=1))\n",
    "            # Need final gamma element for new B\n",
    "            prod =  (alpha[:,T-1] * beta[:,T-1]).reshape((-1,1))\n",
    "            gamma_T = prod/P_O\n",
    "            gamma = np.hstack((gamma,  gamma_T)) #append one Tore to gamma!!!\n",
    "\n",
    "            newpi = gamma[:,0]\n",
    "            newA = np.sum(gi,2) / np.sum(gamma[:,:-1],axis=1).reshape((-1,1))\n",
    "            newB = copy(B)\n",
    "            \n",
    "            sumgamma = np.sum(gamma,axis=1)\n",
    "            for ob_k in range(M):\n",
    "                list_k = observations == ob_k\n",
    "                newB[:,ob_k] = np.sum(gamma[:,list_k],axis=1) / sumgamma\n",
    "\n",
    "            if np.max(abs(pi - newpi)) < criterion and \\\n",
    "                   np.max(abs(A - newA)) < criterion and \\\n",
    "                   np.max(abs(B - newB)) < criterion:\n",
    "                convergence = True;\n",
    "  \n",
    "            A[:],B[:],pi[:] = newA,newB,newpi\n",
    "\n",
    "        self.A[:] = newA\n",
    "        self.B[:] = newB\n",
    "        self.pi[:] = newpi\n",
    "        self.gamma = gamma\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "       \n",
    "    pi = np.array([0.6, 0.4])\n",
    "    A = np.array([[0.7, 0.3],[0.6, 0.4]])\n",
    "    B = np.array([[0.7, 0.1, 0.2],[0.1, 0.6, 0.3]])\n",
    "    hmmguess = HMM(pi,A,B)\n",
    "    print('Viterbi sequence:',hmmguess.ViterbiSequence(np.array([0,1,0,2])))\n",
    "    print('max prob sequence:',hmmguess.maxProbSequence(np.array([0,1,0,2])))    \n",
    "    #obs,states = hmmguess.simulate(4)\n",
    "    hmmguess.train(np.array([0,1,0,2]),0.000001)\n",
    "\n",
    "    print('Estimated initial probabilities:',hmmguess.pi)\n",
    "    print('Estimated state transition probabililities:',hmmguess.A)\n",
    "    print('Estimated observation probabililities:',hmmguess.B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c470103a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
