{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "386a1eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#自然语言处理：LDA，潜在迪利克雷分布分析、意见挖掘、情感分析。。。。。。\n",
    "#索引器：一种网页存储方式，将爬虫爬取到的网页存储到结构化数据库。\n",
    "\n",
    "import os\n",
    "import numpy\n",
    "from bs4 import BeautifulSoup              #读取目录下的文件，使用美味唐解析网页内容\n",
    "moviehtmldir = './data/movie/'\n",
    "moviedict = {}\n",
    "for filename in [f for f in os.listdir(moviehtmldir) if f[0]!='.']:\n",
    "    id = filename.split('.')[0]\n",
    "    f = open(moviehtmldir+'/'+filename,encoding='unicode_escape')\n",
    "    parsed_html = BeautifulSoup(f.read())\n",
    "    try:\n",
    "        title = parsed_html.body.h1.text\n",
    "    except:\n",
    "        title = 'none'\n",
    "    moviedict[id] = title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ef9e2bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [WinError 10054]\n",
      "[nltk_data]     远程主机强迫关闭了一个现有的连接。>\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'29590'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_31348/564781056.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mdir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./data/review_polarity/txt_sentoken/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mpos_textreviews\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpos_titles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mListDocs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'pos/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[0mneg_textreviews\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mneg_titles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mListDocs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'neg/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mtot_textreviews\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpos_textreviews\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mneg_textreviews\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_31348/564781056.py\u001b[0m in \u001b[0;36mListDocs\u001b[1;34m(dirname)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m#分割\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mtitles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmoviedict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mdocs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtitles\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '29590'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "tknzr = WordPunctTokenizer()                      #实例一个对象\n",
    "nltk.download('stopwords')                        #从nltk库中下载停用词\n",
    "stoplist = stopwords.words('english')             #英文停用词   \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "def ListDocs(dirname):                            #对每一个文件解析出来的信息进行处理\n",
    "    docs = []\n",
    "    titles = []\n",
    "    for filename in [f for f in os.listdir(dirname) if str(f)[0]!='.']:#所有不为空的文件\n",
    "        f = open(dirname+'/'+filename,'r')\n",
    "        id = filename.split('.')[0].split('_')[1]  #分割\n",
    "        titles.append(moviedict[id])\n",
    "        docs.append(f.read())\n",
    "    return docs,titles\n",
    "\n",
    "dir = './data/review_polarity/txt_sentoken/'\n",
    "pos_textreviews,pos_titles = ListDocs(dir+'pos/')\n",
    "neg_textreviews,neg_titles = ListDocs(dir+'neg/')\n",
    "tot_textreviews = pos_textreviews+neg_textreviews\n",
    "tot_titles = pos_titles+neg_titles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6928a2b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tot_textreviews' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_31348/975146112.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnewtexts\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mprocessed_reviews\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPreprocessTfidf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtot_textreviews\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstoplist\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#调用预处理函数\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mmod_tfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessed_reviews\u001b[0m\u001b[1;33m)\u001b[0m        \u001b[1;31m#模型拟合\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mvec_tfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmod_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessed_reviews\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tot_textreviews' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def PreprocessTfidf(texts,stoplist=[],stem=False):  #对文档进行预处理(删除停用词、分词、提取词干)\n",
    "    newtexts = []\n",
    "    for text in texts:\n",
    "        if stem:\n",
    "           tmp = [w for w in tknzr.tokenize(text) if w not in stoplist] #文档中不在停用词列表中返回\n",
    "        else:\n",
    "           tmp = [stemmer.stem(w) for w in [w for w in tknzr.tokenize(text) if w not in stoplist]]\n",
    "        newtexts.append(' '.join(tmp))\n",
    "    return newtexts\n",
    "vectorizer = TfidfVectorizer(min_df=1)\n",
    "processed_reviews = PreprocessTfidf(tot_textreviews,stoplist,True)#调用预处理函数\n",
    "mod_tfidf = vectorizer.fit(processed_reviews)        #模型拟合\n",
    "vec_tfidf = mod_tfidf.transform(processed_reviews)\n",
    "tfidf = dict(list(zip(vectorizer.get_feature_names(),vectorizer.idf_)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9499547f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'processed_reviews' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_31348/3591081050.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpickle\u001b[0m             \u001b[1;31m#pickle库，实现python的持久化存储\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#print mod_tfidf.get_feature_names()#将得到的结果保存到文件中\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessed_reviews\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'--'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mmod_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessed_reviews\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data/vectorizer.pk'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'processed_reviews' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle as pickle             #pickle库，实现python的持久化存储 \n",
    "#print mod_tfidf.get_feature_names()#将得到的结果保存到文件中\n",
    "print(len(processed_reviews),'--',len(mod_tfidf.get_feature_names()))\n",
    "v= mod_tfidf.transform(processed_reviews)\n",
    "with open('./data/vectorizer.pk', 'wb') as fin:\n",
    "      pickle.dump(mod_tfidf, fin)    #写入文件\n",
    "file = open(\"vectorizer.pk\",'r')\n",
    "load_tfidf =  pickle.load(file)\n",
    "        \n",
    "print(load_tfidf.transform(PreprocessTfidf([' '.join(['drama'])],stoplist,True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "82009ca6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tot_textreviews' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_31348/2438899875.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m                       \u001b[1;32myield\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtknzr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstoplist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGenSimCorpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtot_textreviews\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstoplist\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[0mdict_corpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mntopics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tot_textreviews' is not defined"
     ]
    }
   ],
   "source": [
    "#使用LSA模型\n",
    "import gensim\n",
    "from gensim import models\n",
    "class GenSimCorpus(object):\n",
    "           def __init__(self, texts, stoplist=[],stem=False):#类的初始化函数\n",
    "               self.texts = texts\n",
    "               self.stoplist = stoplist\n",
    "               self.stem = stem\n",
    "               self.dictionary = gensim.corpora.Dictionary(self.iter_docs(texts, stoplist))\n",
    "               \n",
    "            \n",
    "           def __len__(self):                          #返回文本长度\n",
    "               return len(self.texts)                  \n",
    "           def __iter__(self):\n",
    "               for tokens in self.iter_docs(self.texts, self.stoplist):\n",
    "                   yield self.dictionary.doc2bow(tokens)\n",
    "           def iter_docs(self,texts, stoplist):\n",
    "               for text in texts:\n",
    "                   if self.stem:\n",
    "                      yield (stemmer.stem(w) for w in [x for x in tknzr.tokenize(text) if x not in stoplist])\n",
    "                   else:\n",
    "                      yield (x for x in tknzr.tokenize(text) if x not in stoplist)\n",
    "\n",
    "corpus = GenSimCorpus(tot_textreviews,stoplist,True)\n",
    "dict_corpus = corpus.dictionary\n",
    "ntopics = 10\n",
    "lsi =  models.LsiModel(corpus, num_topics=ntopics, id2word=dict_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "19d7f037",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lsi' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_31348/1628704215.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mU\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlsi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprojection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mu\u001b[0m                     \u001b[1;31m#从模型中返回的Lsi对象得到U、V、S矩阵\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mSigma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meye\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mntopics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlsi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprojection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ms\u001b[0m   \u001b[1;31m#计算得到矩阵V，调用gensim进行稀疏矩阵操作\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mV\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus2dense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlsi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlsi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprojection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlsi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprojection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdict_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict_corpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lsi' is not defined"
     ]
    }
   ],
   "source": [
    "U = lsi.projection.u                     #从模型中返回的Lsi对象得到U、V、S矩阵\n",
    "Sigma = np.eye(ntopics)*lsi.projection.s   #计算得到矩阵V，调用gensim进行稀疏矩阵操作\n",
    "V = gensim.matutils.corpus2dense(lsi[corpus], len(lsi.projection.s)).T / lsi.projection.s\n",
    "dict_words = {}\n",
    "for i in range(len(dict_corpus)):\n",
    "    dict_words[dict_corpus[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "abc5680a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "def PreprocessDoc2Vec(text,stop=[],stem=False):  #与处理数据，要转化为Doc2Vec能够处理的格式\n",
    "    words = tknzr.tokenize(text)\n",
    "    if stem:\n",
    "       words_clean = [stemmer.stem(w) for w in [i.lower() for i in words if i not in stop]]\n",
    "    else:                                        #同样的主要判断是否在停用词中进行处理\n",
    "       words_clean = [i.lower() for i in words if i not in stop]\n",
    "    return words_clean\n",
    "\n",
    "Review = namedtuple('Review','words tags')\n",
    "dir = './data/review_polarity/txt_sentoken/'\n",
    "do2vecstem = False\n",
    "reviews_pos = []\n",
    "cnt = 0                                         #对于文件夹下每一个文档都进行预处理\n",
    "for filename in [f for f in os.listdir(dir+'pos/') if str(f)[0]!='.']:\n",
    "    f = open(dir+'pos/'+filename,'r')\n",
    "    reviews_pos.append(Review(PreprocessDoc2Vec(f.read(),stoplist,do2vecstem),['pos_'+str(cnt)]))\n",
    "    cnt+=1\n",
    "    \n",
    "reviews_neg = []\n",
    "cnt= 0\n",
    "for filename in [f for f in os.listdir(dir+'neg/') if str(f)[0]!='.']:\n",
    "    f = open(dir+'neg/'+filename,'r')\n",
    "    reviews_neg.append(Review(PreprocessDoc2Vec(f.read(),stoplist,do2vecstem),['neg_'+str(cnt)]))\n",
    "    cnt+=1\n",
    "\n",
    "tot_reviews = reviews_pos + reviews_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5ed2b704",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_31348/2578098519.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mvec_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmodel_d2v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDoc2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdm_concat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvec_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#build vocab\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\gensim\\models\\doc2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, documents, corpus_file, vector_size, dm_mean, dm, dbow_words, dm_concat, dm_tag_count, dv, dv_mapfile, comment, trim_rule, callbacks, window, epochs, shrink_windows, **kwargs)\u001b[0m\n\u001b[0;32m    302\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m             \u001b[0mshrink_windows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshrink_windows\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 304\u001b[1;33m             \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    305\u001b[0m         )\n\u001b[0;32m    306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'size'"
     ]
    }
   ],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()          #线程数\n",
    "vec_size = 500\n",
    "model_d2v = Doc2Vec(dm=1, dm_concat=0, size=vec_size, window=10, negative=0, hs=0, min_count=1, workers=cores)\n",
    "        #DM架构为1，隐含层为500，窗口大小为10个单词，negative（负采样），hs（层次）\n",
    "model_d2v.build_vocab(tot_reviews)         #训练模型\n",
    "numepochs= 20\n",
    "for epoch in range(numepochs):\n",
    "    try:\n",
    "        print('epoch %d' % (epoch))\n",
    "        model_d2v.train(tot_reviews)\n",
    "        model_d2v.alpha *= 0.99           #学习速率为0.99\n",
    "        model_d2v.min_alpha = model_d2v.alpha\n",
    "    except (KeyboardInterrupt, SystemExit):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5e2a9ecd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mod_tfidf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_31348/3518067565.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#使用TF-IDF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mquery_vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmod_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPreprocessTfidf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstoplist\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0msims\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery_vec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvec_tfidf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m#cosine_similarity函数将稀疏向量转换为常规向量\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#这是所有与查询词相似的网页\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mod_tfidf' is not defined"
     ]
    }
   ],
   "source": [
    "query = ['science','future','action']  #设置检索的查询词\n",
    "#使用TF-IDF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "query_vec = mod_tfidf.transform(PreprocessTfidf([' '.join(query)],stoplist,True))\n",
    "sims= cosine_similarity(query_vec,vec_tfidf)[0]  #cosine_similarity函数将稀疏向量转换为常规向量\n",
    "#这是所有与查询词相似的网页\n",
    "indxs_sims = sims.argsort()[::-1]\n",
    "for d in list(indxs_sims)[:5]:               #取排序之后前五个最相似的网页\n",
    "    print('sim:',sims[d],' title:',tot_titles[d])\n",
    "\n",
    "#使用LSA模型处理，方法类似\n",
    "def TransformWordsListtoQueryVec(wordslist,dict_words,stem=False):\n",
    "    q = np.zeros(len(list(dict_words.keys())))\n",
    "    for w in wordslist:\n",
    "        if stem:\n",
    "            q[dict_words[stemmer.stem(w)]]=1.   #stemmer.stem(w)词干提取\n",
    "        else:\n",
    "            q[dict_words[w]] = 1.\n",
    "    return q\n",
    "q = TransformWordsListtoQueryVec(query,dict_words,True)#词列表转换为查询向量\n",
    "qk =   np.dot(np.dot(q,U),Sigma)\n",
    "sims = np.zeros(len(tot_textreviews))\n",
    "for d in range(len(V)):\n",
    "    sims[d]=np.dot(qk,V[d])\n",
    "indxs_sims = np.argsort(sims)[::-1]  \n",
    "for d in list(indxs_sims)[:5]:\n",
    "    print('sim:',sims[d],' doc:',tot_titles[d])\n",
    "\n",
    "#doc2vec模型\n",
    "model_d2v.random = np.random.RandomState(1)    #使用RandomState获得随机数生成器\n",
    "query_docvec = model_d2v.infer_vector(PreprocessDoc2Vec(' '.join(query),stoplist,do2vecstem))\n",
    "reviews_related = model_d2v.docvecs.most_similar([query_docvec], topn=5)#model_d2v.docvecs.most_similar([query_docvec], topn=3)\n",
    "for review in reviews_related:\n",
    "    print('relevance:',review[1],'  title:',tot_titles[review[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4661d496",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tot_textreviews' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_31348/1546318531.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[0mnum_topics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m                 \u001b[1;31m#设置主题的个数为10\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGenSimCorpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtot_textreviews\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstoplist\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[0mdict_lda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[0mlda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLdaModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict_lda\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tot_textreviews' is not defined"
     ]
    }
   ],
   "source": [
    "###########################潜在迪利克雷分配(LDA)#################################\n",
    "#一些变量可以用潜在的、未被观察到的变量解释\n",
    "#LDA主题分类示例\n",
    "\n",
    "import gensim.models\n",
    "from gensim import models\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tknzr = RegexpTokenizer(r'((?<=[^\\w\\s])\\w(?=[^\\w\\s])|(\\W))+', gaps=True)\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()                    #词干提取\n",
    "class GenSimCorpus(object):\n",
    "           def __init__(self, texts, stoplist=[],bestwords=[],stem=False):\n",
    "               self.texts = texts   #初始化函数\n",
    "               self.stoplist = stoplist\n",
    "               self.stem = stem\n",
    "               self.bestwords = bestwords\n",
    "               self.dictionary = gensim.corpora.Dictionary(self.iter_docs(texts, stoplist))\n",
    "            \n",
    "           def __len__(self):\n",
    "               return len(self.texts)\n",
    "           def __iter__(self):\n",
    "               for tokens in self.iter_docs(self.texts, self.stoplist):\n",
    "                   yield self.dictionary.doc2bow(tokens)\n",
    "           def iter_docs(self,texts, stoplist):\n",
    "               for text in texts:\n",
    "                   if self.stem:    \n",
    "                      yield (stemmer.stem(w) for w in [x for x in tknzr.tokenize(text) if x not in stoplist])\n",
    "                   else:\n",
    "                      if len(self.bestwords)>0:\n",
    "                         yield (x for x in tknzr.tokenize(text) if x in self.bestwords)\n",
    "                      else:\n",
    "                         yield (x for x in tknzr.tokenize(text) if x not in stoplist)            \n",
    "            \n",
    "num_topics = 10                 #设置主题的个数为10\n",
    "corpus = GenSimCorpus(tot_textreviews, stoplist,[],False)\n",
    "dict_lda = corpus.dictionary\n",
    "lda = models.LdaModel(corpus, num_topics=num_topics, id2word=dict_lda,passes=10, iterations=50)\n",
    "print(lda.show_topics(num_topics=num_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f7ebb3e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dict_lda' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_31348/3537755377.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mout_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtokenid\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtokenid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocfreq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdict_lda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdfs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdocfreq\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1000\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mdocfreq\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m3\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdict_lfq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict_lda\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#深度复制，得到新的独立的对象\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdict_lfq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter_tokens\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_ids\u001b[0m\u001b[1;33m)\u001b[0m     \u001b[1;31m#过滤掉不会给网页增加信息、出现频率高的单词\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdict_lfq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompactify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m    \u001b[1;31m#过滤掉了id，此函数使字典更紧凑\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dict_lda' is not defined"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "out_ids = [tokenid for tokenid, docfreq in dict_lda.dfs.items() if docfreq > 1000 or docfreq < 3 ]\n",
    "dict_lfq = copy.deepcopy(dict_lda)  #深度复制，得到新的独立的对象\n",
    "dict_lfq.filter_tokens(out_ids)     #过滤掉不会给网页增加信息、出现频率高的单词\n",
    "dict_lfq.compactify()    #过滤掉了id，此函数使字典更紧凑\n",
    "corpus = [dict_lfq.doc2bow(tknzr.tokenize(text)) for text in tot_textreviews]\n",
    "\n",
    "lda_lfq = models.LdaModel(corpus, num_topics=num_topics, id2word=dict_lfq,passes=10, iterations=50,alpha=0.01,eta=0.01)\n",
    "for t in range(num_topics):\n",
    "    print('topic ',t,'  words: ',lda_lfq.print_topic(t,topn=10))\n",
    "    print()\n",
    "\n",
    "def GenerateDistrArrays(corpus):     #查询所有属于主题6的电影\n",
    "     for i,dist in enumerate(corpus[:10]):\n",
    "         dist_array = np.zeros(num_topics)\n",
    "         for d in dist:\n",
    "             dist_array[d[0]] =d[1]\n",
    "         if dist_array.argmax() == 6 :\n",
    "            print(tot_titles[i])\n",
    "corpus_lda = lda_lfq[corpus]\n",
    "GenerateDistrArrays(corpus_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "91a6e606",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [WinError 10054]\n",
      "[nltk_data]     远程主机强迫关闭了一个现有的连接。>\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'29590'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_31348/874738077.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'pos/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[0mreviews_pos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mReview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPreprocessReviews\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstoplist\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdo2vecstem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmoviedict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pos_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcnt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m     \u001b[0mcnt\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '29590'"
     ]
    }
   ],
   "source": [
    "#########################观点挖掘（情感分析）####################################\n",
    "#观点挖掘的标准方法是将情感积极或消极作为分类的目标类别\n",
    "#\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tknzr = WordPunctTokenizer()\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import namedtuple\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "#collections模块的namedtuple子类不仅可以使用item的index访问item，还可以通过item的name进行访问\n",
    "tknzr = RegexpTokenizer(r'((?<=[^\\w\\s])\\w(?=[^\\w\\s])|(\\W))+', gaps=True)#正则表达式匹配\n",
    "nltk.download('stopwords')    #导入停用词（手动下载完成）\n",
    "stoplist = stopwords.words('english')#语言为英文\n",
    "stemmer = PorterStemmer()     #初始化对象，用于词干提取\n",
    "\n",
    "def PreprocessReviews(text,stop=[],stem=False):     #预处理函数，对词干进行提取\n",
    "    words = tknzr.tokenize(text)\n",
    "    if stem:\n",
    "       words_clean = [stemmer.stem(w) for w in [i.lower() for i in words if i not in stop]]\n",
    "    else:\n",
    "       words_clean = [i.lower() for i in words if i not in stop]\n",
    "    return words_clean\n",
    "\n",
    "Review = namedtuple('Review','words title tags')\n",
    "dir = './data/review_polarity/txt_sentoken/'\n",
    "do2vecstem = True\n",
    "reviews_pos = []\n",
    "cnt = 0\n",
    "for filename in [f for f in os.listdir(dir+'pos/') if str(f)[0]!='.']: \n",
    "    f = open(dir+'pos/'+filename,'r')\n",
    "    id = filename.split('.')[0].split('_')[1]\n",
    "    reviews_pos.append(Review(PreprocessReviews(f.read(),stoplist,do2vecstem),moviedict[id],['pos_'+str(cnt)]))\n",
    "    cnt+=1\n",
    "    \n",
    "reviews_neg = []\n",
    "cnt= 0\n",
    "for filename in [f for f in os.listdir(dir+'neg/') if str(f)[0]!='.']:\n",
    "    f = open(dir+'neg/'+filename,'r')\n",
    "    id = filename.split('.')[0].split('_')[1]\n",
    "    reviews_neg.append(Review(PreprocessReviews(f.read(),stoplist,do2vecstem),moviedict[id],['neg_'+str(cnt)]))\n",
    "    cnt+=1\n",
    "\n",
    "tot_reviews = reviews_pos + reviews_neg\n",
    "#切分数据及，训练集（80%）、测试机（20%），并且转换数据格式，使其能够被nltk库处理\n",
    "def word_features(words):\n",
    "    return dict([(word, True) for word in words])\n",
    "negfeatures = [(word_features(r.words), 'neg') for r in reviews_neg]#负向特征\n",
    "posfeatures = [(word_features(r.words), 'pos') for r in reviews_pos]#正向特征\n",
    "portionpos = int(len(posfeatures)*0.8)\n",
    "portionneg = int(len(negfeatures)*0.8)\n",
    "print(portionpos,'-',portionneg)\n",
    "trainfeatures = negfeatures[:portionneg] + posfeatures[:portionpos]#训练集\n",
    "print(len(trainfeatures))\n",
    "testfeatures = negfeatures[portionneg:] + posfeatures[portionpos:]#测试集\n",
    "#使用朴素贝叶斯分类器训练和测试分类器\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "classifier = NaiveBayesClassifier.train(trainfeatures) #初始化分类器对象\n",
    "err = 0\n",
    "print('test on: ',len(testfeatures))\n",
    "for r in testfeatures:         #使用朴素贝叶斯对测试集进行分类\n",
    "    sent = classifier.classify(r[0])\n",
    "    if sent != r[1]:\n",
    "       err +=1.\n",
    "print('error rate: ',err/float(len(testfeatures)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "28e6b8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - 800\n",
      "0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'NaiveBayesClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_31348/1941431121.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mtrainfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnegfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mportionpos\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mposfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mportionneg\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNaiveBayesClassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#使用朴素贝叶斯分类器分类\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mtestfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnegfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mportionneg\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mposfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mportionpos\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'NaiveBayesClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "#使用卡方方法选择最佳二元组特征\n",
    "import itertools\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from random import shuffle\n",
    "#用于查找和排列bigram搭配或其他关联度量的工具\n",
    "def bigrams_words_features(words, nbigrams=200,measure=BigramAssocMeasures.chi_sq):\n",
    "    bigram_finder = BigramCollocationFinder.from_words(words)\n",
    "    #把词列表变为双词搭配。为给定序列中的所有bigrams构建一个BigramCollocationFinder。\n",
    "    bigrams = bigram_finder.nbest(measure, nbigrams)\n",
    "    return dict([(ngram, True) for ngram in itertools.chain(words, bigrams)])\n",
    "\n",
    "negfeatures = [(bigrams_words_features(r.words,500), 'neg') for r in reviews_neg]\n",
    "posfeatures = [(bigrams_words_features(r.words,500), 'pos') for r in reviews_pos]\n",
    "portionpos = int(len(posfeatures)*0.8)\n",
    "portionneg = int(len(negfeatures)*0.8)\n",
    "print(portionpos,'-',portionneg)\n",
    "trainfeatures = negfeatures[:portionpos] + posfeatures[:portionneg]\n",
    "print(len(trainfeatures))\n",
    "classifier = NaiveBayesClassifier.train(trainfeatures)#使用朴素贝叶斯分类器分类\n",
    "\n",
    "testfeatures = negfeatures[portionneg:] + posfeatures[portionpos:]\n",
    "shuffle(testfeatures)                #打乱原测试特征的顺序\n",
    "err = 0\n",
    "print('test on: ',len(testfeatures))\n",
    "for r in testfeatures:\n",
    "    sent = classifier.classify(r[0])\n",
    "    if sent != r[1]:\n",
    "       err +=1.\n",
    "print('error rate: ',err/float(len(testfeatures)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "974a1912",
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_31348/3547808576.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfreq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_fd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     pos_score = BigramAssocMeasures.chi_sq(label_word_fd['pos'][word],\n\u001b[1;32m---> 21\u001b[1;33m                 (freq, pos_words), tot_words)\n\u001b[0m\u001b[0;32m     22\u001b[0m     neg_score = BigramAssocMeasures.chi_sq(label_word_fd['neg'][word],\n\u001b[0;32m     23\u001b[0m                 (freq, neg_words), tot_words)\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\nltk\\metrics\\association.py\u001b[0m in \u001b[0;36mchi_sq\u001b[1;34m(cls, n_ii, n_ix_xi_tuple, n_xx)\u001b[0m\n\u001b[0;32m    230\u001b[0m         \"\"\"\n\u001b[0;32m    231\u001b[0m         \u001b[1;33m(\u001b[0m\u001b[0mn_ix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_xi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_ix_xi_tuple\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mn_xx\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mphi_sq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_ii\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mn_ix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_xi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_xx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\nltk\\metrics\\association.py\u001b[0m in \u001b[0;36mphi_sq\u001b[1;34m(cls, *marginals)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m         return (n_ii * n_oo - n_io * n_oi) ** 2 / (\n\u001b[1;32m--> 223\u001b[1;33m             \u001b[1;33m(\u001b[0m\u001b[0mn_ii\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mn_io\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mn_ii\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mn_oi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mn_io\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mn_oo\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mn_oi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mn_oo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m         )\n\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "import nltk.classify.util, nltk.metrics\n",
    "tot_poswords = [val for l in [r.words for r in reviews_pos] for val in l]\n",
    "tot_negwords = [val for l in [r.words for r in reviews_neg] for val in l]\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "word_fd = FreqDist()\n",
    "label_word_fd = ConditionalFreqDist()\n",
    "#计算每个单词在全部语料、积极语料、消极语料词频，抽取前一万\n",
    "for word in tot_poswords:\n",
    "    word_fd[word.lower()] +=1\n",
    "    label_word_fd['pos'][word.lower()] +=1\n",
    "for word in tot_negwords:\n",
    "    word_fd[word.lower()] +=1\n",
    "    label_word_fd['neg'][word.lower()] +=1\n",
    "pos_words = len(tot_poswords)\n",
    "neg_words = len(tot_negwords)\n",
    "tot_words = pos_words + neg_words\n",
    "word_scores = {}\n",
    " \n",
    "for word, freq in word_fd.items():\n",
    "    pos_score = BigramAssocMeasures.chi_sq(label_word_fd['pos'][word],\n",
    "                (freq, pos_words), tot_words)\n",
    "    neg_score = BigramAssocMeasures.chi_sq(label_word_fd['neg'][word],\n",
    "                (freq, neg_words), tot_words)\n",
    "    word_scores[word] = pos_score + neg_score\n",
    "print('total: ',len(word_scores))\n",
    "best = sorted(iter(word_scores.items()), key=lambda w_s: w_s[1], reverse=True)[:10000]\n",
    "bestwords = set([w for w, s in best])\n",
    "\n",
    "def best_words_features(words):\n",
    "    return dict([(word, True) for word in words if word in bestwords])\n",
    "\n",
    "negfeatures = [(best_words_features(r.words), 'neg') for r in reviews_neg]\n",
    "posfeatures = [(best_words_features(r.words), 'pos') for r in reviews_pos]\n",
    "portionpos = int(len(posfeatures)*0.8)\n",
    "portionneg = int(len(negfeatures)*0.8)\n",
    "print(portionpos,'-',portionneg)\n",
    "trainfeatures = negfeatures[:portionpos] + posfeatures[:portionneg]\n",
    "print(len(trainfeatures))\n",
    "classifier = NaiveBayesClassifier.train(trainfeatures)\n",
    "##test with feature chi square selection\n",
    "testfeatures = negfeatures[portionneg:] + posfeatures[portionpos:]\n",
    "shuffle(testfeatures)\n",
    "err = 0\n",
    "print('test on: ',len(testfeatures))\n",
    "for r in testfeatures:\n",
    "    sent = classifier.classify(r[0])\n",
    "    #print r[1],'-pred: ',sent\n",
    "    if sent != r[1]:\n",
    "       err +=1.\n",
    "print('error rate: ',err/float(len(testfeatures)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3063f660",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
